QUICK START

On a theta login node, under your desired directory, run the following:
	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 10 -n 2 -q debug-cache-quad \
		run-example SparkPi

On a cooley login node, use the debug queue instead:
	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 10 -n 2 -q debug \
		run-example SparkPi

The result output will be under the current directory.


USER SETUP

The file, env_theta.sh or env_cooley.sh, contains preset configurations
for either machine.  To override these preset, create a file,
env_local.sh, under your current directory, where you launch
submit-spark.sh.  Note that the file, env_local.sh, will be sourced
by bash mutiple times.  You can change the default output directory,
where env_local.sh should reside, by using the `-o` option to
submit-spark.sh.

You can get more help by issuing:
	$ /PATH/TO/SPARK_JOB/submit-spark.sh -h


EXAMPLES

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug-cache-quad run-example SparkPi
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug-cache-quad` queue.  The job will run the scala example
`SparkPi` came with the default install of apache spark.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug $SPARK_HOME/examples/src/main/python/pi.py 10000
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will run the pyspark example `pi.py`
came with the default install of apache spark.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug -s SomeExe Args
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will run the executable `SomeExe` with
arguments `Args` on the compute node that has the spark master
running.  Spark related variables will populate the running
environment.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q pubnet-debug -w 10
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will drop in to a shell environment on
the compute node of the spark master.


BUGS

Paths or environment variables containing quotes may break the scripts.
