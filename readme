QUICK START

On a theta login node, under your desired directory, replace
`datascience` with your allocation, run the following:
	$ /soft/datascience/Spark_Job/submit-spark.sh \
		-A datascience -t 10 -n 2 -q debug-cache-quad \
		run-example SparkPi

On a cooley login node, under your desired directory, replace
`datascience` with your allocation, run the following:
	$ /soft/datascience/Cooley_Spark/submit-spark.sh \
		-A datascience -t 10 -n 2 -q debug \
		run-example SparkPi

The result output will be under the current directory.


SPARK JOB

Spark Job is a set of scripts that interfaces with Cobalt and
automates the job submission process so you can just use Apache
Spark and focus on your real work.

Currently the scripts are installed under the directory,
	/soft/datascience/Spark_Job		on Theta
or
	/soft/datascience/Cooley_Spark		on Cooley
In the following, we will call the installation directory,
`/PATH/TO/SPARK_JOB`.

The main user interface is the file,
	/PATH/TO/SPARK_JOB/submit-spark.sh

You can get more help by issuing:
	$ /PATH/TO/SPARK_JOB/submit-spark.sh -h


USER SETUP

The file, env_theta.sh or env_cooley.sh, contains preset configurations
for either machine.  To override these preset, create a file,
env_local.sh, under your current directory, where you launch
submit-spark.sh.  Note that the file, env_local.sh, will be sourced
by bash mutiple times.  You can change the default output directory,
where env_local.sh should reside, by using the `-o` option to
submit-spark.sh.

You can find more documentation in the directory `example`, along
with a template `env_local.sh` file.


EXAMPLE SUBMIT COMMANDS

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug-cache-quad run-example SparkPi
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug-cache-quad` queue.  The job will run the scala example
`SparkPi` came with the default install of apache spark.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug $SPARK_HOME/examples/src/main/python/pi.py 10000
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will run the pyspark example `pi.py`
came with the default install of apache spark.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q debug -s SomeExe Args
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will run the executable `SomeExe` with
arguments `Args` on the compute node that has the spark master
running.  Spark related variables will populate the running
environment.

	$ /PATH/TO/SPARK_JOB/submit-spark.sh \
		-A datascience -t 60 -n 2 -q pubnet-debug -w 10
The script will submit a COBALT job using the `datascience` allocation,
for a maximum wall clock time of 60 minutes, request 2 nodes, using
the `debug` queue.  The job will drop in to a shell environment on
the compute node of the spark master.


BUGS

Paths or environment variables containing quotes may break the scripts.
